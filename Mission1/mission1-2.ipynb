{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23514,"status":"ok","timestamp":1727611078843,"user":{"displayName":"이현승","userId":"01495869685552383607"},"user_tz":-540},"id":"3QHnS326ZUFi","outputId":"d3958ad9-cc7e-4b55-8c3e-6f1fbcf02025"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","import sys\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUfTLR6QYmEA"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","# from categorize_img import categorize_imgs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4SugGChZoCj"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 데이터 전처리\n","# 성별 & 스타일을 분류하는 작업이므로, 데이터셋에 적합한 전처리 과정을 거칩니다.\n","# 간단한 object detection과 같은 전처리는 제외하고, 기본적인 이미지 크롭과 augmentation을 사용.\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # ResNet의 입력 크기에 맞춤\n","    transforms.RandomHorizontalFlip(),  # 데이터 증강\n","    transforms.RandomCrop(224, padding=4),  # 랜덤 크롭\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet 통계치 사용\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":634850,"status":"ok","timestamp":1727610641582,"user":{"displayName":"이현승","userId":"01495869685552383607"},"user_tz":-540},"id":"EYUFw-xClI3M","outputId":"fd61a642-0125-4ea2-d6d9-1790136a4dbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Images are categorized successfully! ==> Total images processed: 1799 / Total images failed: 0\n","Images are categorized successfully! ==> Total images processed: 951 / Total images failed: 0\n"]}],"source":["import shutil\n","\n","def categorize_imgs(src, dest_dir):\n","    # Google Drive 내 src_folder 경로 지정\n","    src_folder = src\n","\n","    # dest_folder 지정(폴더가 없다면 생성)\n","    dest_folder = os.path.join(dest_dir, os.path.basename(src))\n","    if not os.path.exists(dest_folder):\n","        os.makedirs(dest_folder)\n","\n","    # 오류 발생을 대비하여 처리된 이미지 수 추적\n","    processed_count = 0\n","    error_count = 0\n","\n","    # src_folder의 모든 파일을 가져옴\n","    for filename in os.listdir(src_folder):\n","        if filename.endswith('.jpg'):\n","            try:\n","                # 파일명에서 style 정보 추출\n","                style = filename.split('_')[-2]\n","\n","                # style에 해당하는 폴더가 없다면 생성\n","                style_folder = os.path.join(dest_folder, style)\n","                if not os.path.exists(style_folder):\n","                    os.makedirs(style_folder)\n","\n","                # 원본 이미지 복사(copy2: metadata까지 복사)\n","                src_path = os.path.join(src_folder, filename)\n","                dest_path = os.path.join(style_folder, filename)\n","                shutil.copy2(src_path, dest_path)\n","\n","                # 정상적으로 처리하였을 경우 count 증가\n","                processed_count += 1\n","\n","            except Exception as e:\n","                # 에러가 발생하여 변환에 실패하면 에러 메시지 출력\n","                error_count += 1\n","                print(f\"Error processing {filename}: {str(e)}\")\n","\n","    print(f\"Images are categorized successfully! ==> Total images processed: {processed_count} / Total images failed: {error_count}\")\n","\n","\n","if __name__ == \"__main__\":\n","    # Google Drive에 저장될 폴더 경로 지정\n","    dest_dir = '/content/drive/MyDrive/categorized_data'\n","\n","    # 새로 만들어질 폴더가 없다면 생성\n","    if not os.path.exists(dest_dir):\n","        os.makedirs(dest_dir)\n","\n","    # source data가 있는 Google Drive 내 폴더 지정\n","    train_path = '/content/drive/MyDrive/dataset/training_image'\n","    val_path = '/content/drive/MyDrive/dataset/validation_image'\n","\n","    # style 별로 이미지 분류\n","    categorize_imgs(train_path, dest_dir)\n","    categorize_imgs(val_path, dest_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rX7YWRL8ZqUp"},"outputs":[],"source":["# 데이터 로드\n","train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/categorized_data/training_image', transform=transform)\n","val_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/categorized_data/validation_image', transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727611101017,"user":{"displayName":"이현승","userId":"01495869685552383607"},"user_tz":-540},"id":"Egyh8cdsolI3","outputId":"c3da290a-bca8-4e6f-94ef-74f924046662"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'athleisure': 0, 'bodyconscious': 1, 'bold': 2, 'cityglam': 3, 'classic': 4, 'disco': 5, 'ecology': 6, 'feminine': 7, 'genderless': 8, 'grunge': 9, 'hiphop': 10, 'hippie': 11, 'ivy': 12, 'kitsch': 13, 'lingerie': 14, 'lounge': 15, 'metrosexual': 16, 'military': 17, 'minimal': 18, 'mods': 19, 'normcore': 20, 'oriental': 21, 'popart': 22, 'powersuit': 23, 'punk': 24, 'space': 25, 'sportivecasual': 26}\n","{'athleisure': 0, 'bodyconscious': 1, 'bold': 2, 'cityglam': 3, 'classic': 4, 'disco': 5, 'ecology': 6, 'feminine': 7, 'genderless': 8, 'grunge': 9, 'hiphop': 10, 'hippie': 11, 'ivy': 12, 'kitsch': 13, 'lingerie': 14, 'lounge': 15, 'metrosexual': 16, 'military': 17, 'minimal': 18, 'mods': 19, 'normcore': 20, 'oriental': 21, 'popart': 22, 'powersuit': 23, 'punk': 24, 'space': 25, 'sportivecasual': 26}\n"]}],"source":["# 데이터셋의 클래스 수 확인\n","print(train_dataset.class_to_idx)\n","print(val_dataset.class_to_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1344,"status":"ok","timestamp":1727611130883,"user":{"displayName":"이현승","userId":"01495869685552383607"},"user_tz":-540},"id":"UqhhL2j9ZvTg","outputId":"0030c54e-5319-4822-cde4-73f052f0e798"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}],"source":["# 모델 초기화 (ResNet-18)\n","# 사전 학습된 가중치 없이 랜덤으로 초기화\n","# resnet-18 모델을 전이학습 없이 직접 구현\n","import torch\n","import torch.nn as nn\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != self.expansion * out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * out_channels)\n","            )\n","\n","    def forward(self, x):\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = self.relu(out)\n","        return out\n","\n","class ResNet18(nn.Module):\n","    def __init__(self, num_classes=27):\n","        super(ResNet18, self).__init__()\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(BasicBlock, 64, 2)\n","        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n","        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n","        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n","\n","    def _make_layer(self, block, out_channels, blocks, stride=1):\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride))\n","        self.in_channels = out_channels * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.in_channels, out_channels))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","# 모델 생성\n","model = ResNet18()\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JjOFw2qbFm9"},"outputs":[],"source":["# 손실 함수와 옵티마이저 정의\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHdfi2FcbIJw"},"outputs":[],"source":["import torch\n","\n","# 학습 함수 정의 (학습과 검증 분리)\n","def train_model(model, criterion, optimizer, train_loader, num_epochs=10, save_path='./model_epoch'):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        running_corrects = 0\n","        total_train_samples = 0\n","\n","        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n","        print('-' * 20)\n","\n","        # 학습 단계 (배치마다 결과 출력)\n","        for batch_idx, (inputs, labels) in enumerate(train_loader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            _, preds = torch.max(outputs, 1)\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","            total_train_samples += inputs.size(0)\n","\n","            # 배치마다 결과 출력\n","            batch_loss = loss.item()\n","            batch_acc = torch.sum(preds == labels.data).double() / inputs.size(0)\n","            print(f'Batch {batch_idx+1}/{len(train_loader)}: Train Loss: {batch_loss:.4f}, Train Acc: {batch_acc:.4f}')\n","\n","        # 에포크 결과 계산\n","        epoch_loss = running_loss / total_train_samples\n","        epoch_acc = running_corrects.double() / total_train_samples\n","        print(f'Epoch {epoch+1} Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')\n","\n","        # 에포크마다 모델의 state_dict 저장\n","        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n","\n","def validate_model(model, criterion, val_loader, load_path):\n","    # 저장된 state_dict 로드\n","    model.load_state_dict(torch.load(load_path))\n","    model.eval()\n","\n","    val_running_loss = 0.0\n","    val_corrects = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (val_inputs, val_labels) in enumerate(val_loader):\n","            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n","            val_outputs = model(val_inputs)\n","            val_loss = criterion(val_outputs, val_labels)\n","            val_running_loss += val_loss.item() * val_inputs.size(0)\n","\n","            _, val_preds = torch.max(val_outputs, 1)\n","            val_corrects += torch.sum(val_preds == val_labels.data)\n","            val_total += val_labels.size(0)\n","\n","            # 배치마다 결과 출력\n","            val_batch_loss = val_loss.item()\n","            val_batch_acc = torch.sum(val_preds == val_labels.data).double() / val_inputs.size(0)\n","            print(f'Batch {batch_idx+1}/{len(val_loader)}: Val Loss: {val_batch_loss:.4f}, Val Acc: {val_batch_acc:.4f}')\n","\n","    # 에포크 결과 계산\n","    val_epoch_loss = val_running_loss / val_total\n","    val_epoch_acc = val_corrects.double() / val_total\n","    print(f'Validation Loss: {val_epoch_loss:.4f}, Validation Acc: {val_epoch_acc:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NE1JJ02bJ8G"},"outputs":[],"source":["# 모델 상태를 저장할 파일 경로\n","save_path = '/content/drive/MyDrive/model'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRmpQ0FRxqy0"},"outputs":[],"source":["# 학습 실행\n","train_model(model, criterion, optimizer, train_loader, num_epochs=10, save_path=save_path)"]},{"cell_type":"code","source":["# 학습 완료 후 검증 단계 호출\n","validate_model(model, criterion, val_loader, load_path='/content/drive/MyDrive/model/model_epoch_10.pth')"],"metadata":{"id":"QZ_Hq5Hv6vnc"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}